{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:17:37.372674Z",
     "start_time": "2019-10-15T06:17:37.368674Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_content(path, threshold = False):\n",
    "    '''\n",
    "    threshold: if is True, it's represent document that filter the first between third lines,\n",
    "               because it's noise.\n",
    "    result: cleaned content\n",
    "    '''\n",
    "    #Step 1.1 open document or query\n",
    "    content = open(path, 'r')\n",
    "    #Step 1.2 read content line by line\n",
    "    lines = content.read().splitlines()\n",
    "    #Step 1.3 decide is document or query\n",
    "    if threshold is True:\n",
    "        content = lines[3:]\n",
    "    else:\n",
    "        content = lines\n",
    "    #Step 1.4 added all of lines in result\n",
    "    result = ''\n",
    "    for i in range(len(content)):\n",
    "        result = result + content[i].strip('-1')    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:17:37.914670Z",
     "start_time": "2019-10-15T06:17:37.911670Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_lexicon(N, document): \n",
    "    '''\n",
    "    lexicon: is word dictionary which including all of document and it stored index.\n",
    "    N: docu_clean length\n",
    "    '''\n",
    "    #Step 3.1 create lexicon dictionary and index\n",
    "    lexicon = {}\n",
    "    count = 0\n",
    "    #Step 3.2 decide if word not in the lexicon then update lexicon key and stored index\n",
    "    for i in range(N):\n",
    "        if document[i] not in lexicon:\n",
    "            lexicon.update({document[i]: count})\n",
    "            count += 1\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:59:17.103505Z",
     "start_time": "2019-10-15T06:59:17.098506Z"
    }
   },
   "outputs": [],
   "source": [
    "def tf_frequency(document, lexicon):\n",
    "    '''\n",
    "    tf: term frequency(row is document length, column is lexicon length)\n",
    "    lexicon: word dictionary\n",
    "    document: all document or query\n",
    "    '''\n",
    "    #Step 4.1 create tf 2d-array\n",
    "    tf = np.zeros((len(document), len(lexicon)))\n",
    "    #Step 4.2 judged tf shape\n",
    "    assert tf.shape == (len(document), len(lexicon)), 'tf shape error'\n",
    "    #Step 4.3 map each documents\n",
    "    for n in range(len(document)):\n",
    "        #Step 4.4 map each words of document\n",
    "        for word in document[n].split():\n",
    "            #Step 4.5 determine which word in lexicon existed\n",
    "            if (word in lexicon):\n",
    "                #finded this word index and in tf[this document][this word index] + 1\n",
    "                idx = lexicon[word]\n",
    "                tf[n][idx] += 1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:23:34.220504Z",
     "start_time": "2019-10-15T07:23:34.215504Z"
    }
   },
   "outputs": [],
   "source": [
    "def idf_transform(N, tf):\n",
    "    '''\n",
    "    N: lexicon length\n",
    "    tf: document tf\n",
    "    '''\n",
    "    #Step 5.1 create tf 1d-vector that type is float\n",
    "    idf = np.zeros(N, dtype = float)\n",
    "    #Step 5.2 judge idf shape\n",
    "    assert idf.shape[0] == tf.shape[1], 'idf shape error'\n",
    "    #Step 5.3 map lexicon word\n",
    "    for word in range(N):\n",
    "        n = 0\n",
    "        #Step 5.4 map each document words and calculate it's idf\n",
    "        for d in range(tf.shape[0]):\n",
    "            if (tf[d][word] > 0):\n",
    "                n += 1\n",
    "        #Step 5.5 calculate idf \n",
    "        idf[word] = np.log((N - n + 0.5) / (n + 0.5))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:23:36.883508Z",
     "start_time": "2019-10-15T07:23:36.878504Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_tfij_(N, b, document, tf):\n",
    "    #Step 6.1 used advanced that calculated each document length\n",
    "    dl = [len(document[i].split()) for i in range(len(document))]\n",
    "    #Step 6.2 computed average document\n",
    "    avg_dl = np.divide(np.sum(dl), N)\n",
    "    #Step 6.3 computed K\n",
    "    K = (1 - b) + (b * (dl / avg_dl))\n",
    "    #reshape K shape (1d-vector â†’ 2d-array)\n",
    "    K = np.reshape(K, (K.shape[0], 1))\n",
    "    #Step 6.4 computed tfi,j'\n",
    "    tfij_ = np.divide(tf, K)\n",
    "    return tfij_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:23:37.548505Z",
     "start_time": "2019-10-15T07:23:37.542506Z"
    }
   },
   "outputs": [],
   "source": [
    "def bm25_score(k1, k3, **kwargs):\n",
    "    '''\n",
    "    query: query content\n",
    "    document: document content\n",
    "    lexicon: word dictionary\n",
    "    tfij_: tfi,j'\n",
    "    tf_query: tfi,q\n",
    "    idf: idf\n",
    "    '''\n",
    "    #Step 7.1 receive dictionary type data \n",
    "    query = kwargs['query']\n",
    "    document = kwargs['document']\n",
    "    lexicon = kwargs['lexicon']\n",
    "    tfij_ = kwargs['tfij_']\n",
    "    tf_query = kwargs['tf_query']\n",
    "    idf = kwargs['idf']\n",
    "    #Step 7.2 create score 2d-array that row is query length, column is document length\n",
    "    score = np.zeros((len(query),len(document)))\n",
    "    #Step 7.3 judge score shape\n",
    "    assert score.shape == (len(query), len(document)), 'score shape error'\n",
    "    #Step 7.4 map each query\n",
    "    for qlen in range(len(query)):\n",
    "        #Step 7.5 map each documnet\n",
    "        for dlen in range(len(document)):\n",
    "            #Step 7.6 filter word in document and query appear simultaneously\n",
    "            intersection = set(document[dlen].split()) & set(query[qlen].split())\n",
    "            for word in intersection:\n",
    "                idx = lexicon[word]\n",
    "                #Step 7.7 computed ((k1 + 1) * tfi,j'[document][word index]) / (tfi,j'[document][word index] + k1)\n",
    "                B = np.divide((k1 + 1) * tfij_[dlen][idx], (tfij_[dlen][idx] +  k1))\n",
    "                #Step 7.8 computed ((k3 + 1) * tfi,q[query][word index])) / (k3 + tfi,q[query][word index])\n",
    "                G = np.divide((k3 + 1) * tf_query[qlen][idx], (k3 + tf_query[qlen][idx]))\n",
    "                #Step 7.9 computed B * G * idf[index] in score[query][document]\n",
    "                score[qlen][dlen] += (B * G * idf[idx])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:23:40.425506Z",
     "start_time": "2019-10-15T07:23:40.422503Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_sort(N, docu_files, score):\n",
    "    '''\n",
    "    N: query length\n",
    "    docu_files: document filenames\n",
    "    score: simularity in each document\n",
    "    '''\n",
    "    lis = {}\n",
    "    for q in range(N):\n",
    "        #Step 8.1 sorted each document score in this query\n",
    "        a = np.argsort(-score[q])\n",
    "        #Step 8.2 declare lis[q + 1] and content is null\n",
    "        lis['%s'% (q + 1)] = []\n",
    "        #Step 8.3 append to list\n",
    "        for i in range(len(a)):\n",
    "            lis['%s'% (q + 1)].append(docu_files[a[i]])\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:23:40.850508Z",
     "start_time": "2019-10-15T07:23:40.846505Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_text(quer_files, document, lis):\n",
    "    '''\n",
    "    quer_files: query filenames\n",
    "    document: document content\n",
    "    lis: sorted result\n",
    "    '''\n",
    "    #Step 10.1 write into submission.txt\n",
    "    with open('submission.txt', 'a', encoding = 'utf-8') as f:\n",
    "        f.write('Query,RetrievedDocuments\\n')\n",
    "        for i in range(len(quer_files)):\n",
    "            f.write(quer_files[i] + ',')\n",
    "            for v in lis['%s' % (i + 1)]:\n",
    "                f.write(v + ' ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:52:38.428507Z",
     "start_time": "2019-10-15T07:52:38.421507Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def main():\n",
    "    #Step 0. Get document, query path and filenames\n",
    "    document_path = os.path.abspath('Document')\n",
    "    query_path = os.path.abspath('Query')\n",
    "    docu_files = os.listdir(document_path)\n",
    "    quer_files = os.listdir(query_path)\n",
    "    \n",
    "    #Step 1. Read document and query content\n",
    "    document = []\n",
    "    for i in range(len(docu_files)):\n",
    "        document.append(read_content('Document\\\\' + docu_files[i], threshold = True))\n",
    "\n",
    "    query = []\n",
    "    for j in range(len(quer_files)):\n",
    "        query.append(read_content('Query\\\\' + quer_files[j]))\n",
    "    \n",
    "    #Step 2. used regular expression to get words from all of document and query\n",
    "    docu_clean = [re.sub(r\"[^0-9]+\", ' ', document[i].split()[j]) for i in range(len(document)) for j in range(len(document[i].split()))]\n",
    "    \n",
    "    #Step 3. build word dictionary\n",
    "    lexicon = word_lexicon(len(docu_clean), docu_clean)\n",
    "    #Step 4. computed term-frequency of document and query \n",
    "    tf = tf_frequency(document, lexicon)\n",
    "    tf_query = tf_frequency(query, lexicon)\n",
    "    #Step 5. computed intrieve-document-frequency of document\n",
    "    idf = idf_transform(len(lexicon), tf)\n",
    "    \n",
    "    N = len(document)    \n",
    "    k1 = 5\n",
    "    k3 = 1000\n",
    "    b = 0.75\n",
    "    #Step 6. computed term-frequecny i,j'\n",
    "    tfij_ = compute_tfij_(N, b, document, tf)\n",
    "    #Step 7. computed each document simularity of each query\n",
    "    score = bm25_score(k1, k3, query = query, document = document, lexicon = lexicon, tfij_ = tfij_,\\\n",
    "                       tf_query = tf_query, idf = idf)\n",
    "    #Step 8. sotred the score\n",
    "    sort_score = score_sort(len(query), docu_files, score)\n",
    "    #Step 9. judge which is existed or not\n",
    "    if os.path.exists('submission.txt'):\n",
    "        os.remove('submission.txt')\n",
    "    #Step 10. write sorted result into submission.txt\n",
    "    write_text(quer_files, document, sort_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:53:08.443505Z",
     "start_time": "2019-10-15T07:52:38.628508Z"
    }
   },
   "outputs": [],
   "source": [
    "#Executing main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ShenWen]",
   "language": "python",
   "name": "conda-env-ShenWen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
